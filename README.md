# deep_autoencoder
Deep belief network autoencoder

This deep belief network autoencoder is based on the work of Ruslan Salakhutdinov and Geoff Hinton (https://science.sciencemag.org/content/313/5786/504.full) and the associated MATLAB code (http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html). I've translated this into PyTorch and incorporated GPU computations to make it run faster.

The operation is simple. Initialize a DBN object with a number of restricted Boltzmann machine layers, e.g., dbn = DBN(visible_units=512, hidden_units=[256, 128]) will initialize a DBN with 512 input neurons, and two RBM layers, one with 256 output neurons, and one with 128 output neurons. The network is then pretrained, e.g., dbn.pretrain(data, labels, num_epochs), where data is a torch.Tensor of size (num_samples x num_dimensions), labels is a torch.Tensor of the labels of size (num_samples), and num_epochs is an integer denoting how many epochs to pretrain each RBM layer. Next, the network is fine-tuned, e.g., dbn.fine_tuning(data, labels, num_epochs, max_iter), where data, labels are the same as in the previous step, num_epochs is the number of epochs to train the network, and max_iter denotes the number of iterations through the optimization routine per data batch. The fine tuning step first "unrolls" the network, so a 2-layer network becomes a 4-layer network, a 5-layer network becomes a 10-layer network, etc. Then the unrolled network weights are optimized using a line-search algorithm (LBFGS).
